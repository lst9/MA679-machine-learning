---
title: "HW1 Linear Regression"
date: "1/29/2022"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load("knitr","arm","ggplot2","rstanarm")
```

## 3.1
Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

### Answer: 

H0: all coefficients are equal to zero.

We can see all factors' p values are small except newspaper, which means we can not believe advertising on newspapers has an impact on product sales in specific confidence level, while TV and radio have significant relationship with sales.


## 3.2 
Carefully explain the differences between the KNN classifier and KNN regression methods.

### Answer: 

KNN classifier estimates the most possible class $x_0$ will be by detecting K observations closest to $x_0$, KNN regression estimates the value of response by computing average responses of K observations.


## 3.5
Consider the fitted values that result from performing linear regres- sion without an intercept. In this setting, the ith fitted value takes the form $$ \hat{y_i}=x_i \hat{\beta},$$ where $$ \hat{\beta} = (\sum_{i = 1}^n x_i y_i)/(\sum_{i' = 1}^n x_{i'}^2)$$ Show that we can write $$ \hat{y_i} = \sum_{i' = 1}^n a_{i'} y_{i'}.$$ what is $a_{i'}$？

Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.

### Answer: 

$$ \hat{y_i}=x_i (\sum_{i=1}^n x_i y_i)/(\sum_{i'=1}^n x_{i'}^2) =x_i \cdot \sum_{i''=1}^n(x_{i''} y_{i''}/ \sum_{i'=1}^n x_{i'}^2 ) = \sum_{i''=1}^n (x_{i''} x_i/ \sum_{i'=1}^n x_{i'}^2) y_{i''} $$
so, $$ a_{i'}=(x_{i''}/ \sum_{i'=1}^n x_{i'}^2) \cdot x_i $$.


## 3.6
Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar{x}, \bar{y})$.

### Answer: 

From (3.4), we know for $$ \hat{y_i}=\hat{\beta_0}+\hat{\beta_1} x $$, $$ \hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x} $$, so when $x_i$ takes $\bar{x}$, $$ \hat{y_i}=\bar{y}-\hat{\beta_1}\bar{x}+\hat{\beta_1}x=\bar{y} $$. 

So in the case of simple linear regression, the least squares line always passes through the point $(\bar{x}, \bar{y})$.


## 3.11

In this problem we will investigate the t-statistic for the null hypothesis H0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.
```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
```

### (a)

Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate βˆ, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis H0 : β = 0. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)

```{r,echo=FALSE}
lm <- lm(y~x+0)
summary(lm)
```
#### Answer:

The coefficient estimate $\hat{\beta}$ is 1.99, its standard error is 0.1, t value is 18.73, P value is <2e-16, which means this coefficient is significant differ from 0.


### (b)

Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis H0 : β = 0. Comment on these results.

#### Answer:
```{r,echo=FALSE}
lm2 <- lm(x~y+0)
summary(lm2)
```
The coefficient estimate $\hat{\beta}$ is 0.39, its standard error is 0.02, t value is 18.73, P value is <2e-16, which means this coefficient is significant differ from 0.


###(c)

What is the relationship between the results obtained in (a) and (b)?

#### Answer:

Their t value are same, so $$\beta_x \cdot \sum_{i=1}^n x_i^2 = \beta_y \cdot \sum_{i=1}^n y_i^2$$


### (d)

#### Answer:

From OLS, we know $$ \hat{\beta}=\sum_{i=1}^n (x_i- \bar{x})(y_i- \bar{y}) / \sum_{i=1}^n(x_i- \bar{x})^2 $$, here $\bar{x}, \bar{y}$ equal to zero.

So $$ t=\hat{\beta}/ se(\hat{\beta}) $$,
$$ t= (\sum_{i=1}^n x_i y_i / \sum_{i=1}^n x_i^2) \cdot \frac{\sqrt{(n-1)\cdot \sum_{i=1}^n x_i^2}}{\sqrt{\sum_{i=1}^n (y_i-x_i \cdot \frac{\sum_{i'=1}^n x_{i'}y_{i'}}{\sum_{i'=1}^n x_{i'}^2})}} $$
$$ t= \frac{\sqrt{n-1} \cdot \sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2 \cdot \sum_{i'=1}^n y_{i'}^2 - 2(\sum_{i'=1}^n x_{i'}y_{i'})^2+(\sum_{i'=1}^n x_{i'}y_{i'})^2}} =\frac{\sqrt{n-1} \cdot \sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2 \cdot \sum_{i'=1}^n y_{i'}^2 - (\sum_{i'=1}^n x_{i'}y_{i'})^2}} $$
```{r}
t <- (sqrt(100-1)*sum(x*y))/(sqrt(sum(x^2)*sum(y^2)-(sum(x*y))^2))
t
```

### (e)

#### Answer:

Because x,y have same position in the function from (d), so the t-statistic will be same.


### (f)

#### Answer:

```{r}
t_x <- (sqrt(100-1)*sum(x*y))/(sqrt(sum(x^2)*sum(y^2)-(sum(x*y))^2))
t_y <- (sqrt(100-1)*sum(x*y))/(sqrt(sum(y^2)*sum(x^2)-(sum(x*y))^2))
t_x == t_y
```


## 3.12

### (a)
Recall that the coefficient estimate βˆ for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X?

#### Answer:

$$ \sum_{i=1}^n x_i^2 = \sum_{i=1}^n y_i^2$$

### (b)
Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X.

#### Answer:

```{r}
set.seed(2022)
x <- rnorm(100,0,1)
y <- rnorm(100,0,2)
summary(lm(y~x+0))
summary(lm(x~y+0))
```


### (c)
Generate an example in R with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X.
 
#### Answer:

```{r}
set.seed(2022)
x <- rnorm(100,0,1)
y <- rnorm(100,0,1)
summary(lm(y~x+0))
summary(lm(x~y+0))
```


## 3.13

### (a)

```{r}
set.seed(1)
x <- rnorm(100,0,1)
```

### (b)

```{r}
set.seed(100)
eps <- rnorm(100,0,0.5)
```

### (c)

```{r}
y <- -1+0.5*x+eps
```
The length od y is 100, the $\beta_0$ is -1, $\beta_1$ is 0.5.

### (d)

```{r}
ggplot(data.frame(x,y),aes(x=x,y=y))+
  geom_point(position = "jitter")
```

### (e)

```{r}
lm_12 <- lm(y~x)
summary(lm_12)
```
$\hat{\beta_0}$ is -1, $\hat{\beta_1}$ is 0.52, they are very close to real ${\beta_0}$ and ${\beta_1}$.


### (f)

```{r}
ggplot(data.frame(x,y),aes(x=x,y=y))+
  geom_point(position = "jitter")+
  geom_abline(intercept = coef(lm_12)[1], slope = coef(lm_12)[2])+
  geom_abline(intercept = -1, slope = 0.5, color="blue")
#legend(x=-1,legend=c("least sqares line", "population regression line"),col=c("black", "blue"))
```


### (g)

```{r}
x2 <- x^2
lm_12pol <- lm(y~x+x2)
summary(lm_12pol)
```
The R-squared is 0.445, the original regression's R-squared is 0.45, so the quadratic term does not improve the model fit.


### (h)

```{r}
set.seed(2)
xl <- rnorm(100,0,1)
epsl <- rnorm(100,0,0.01)
yl <- -1+0.5*xl+epsl

lm_12l <- lm(yl~xl)
summary(lm_12l) 

x2l <- xl^2
lm_12poll <- lm(yl~xl+x2l)
summary(lm_12poll)
```
When decreasing the noise, the R square in two regressions are improved a lot.


### (i)

```{r}
set.seed(3)
xh <- rnorm(100,0,1)
epsh <- rnorm(100,0,1)
yh <- -1+0.5*xh+epsh

lm_12h <- lm(yh~xh)
summary(lm_12h) 

x2h <- xh^2
lm_12polh <- lm(yh~xh+x2h)
summary(lm_12polh)
```
When increasing the noise, the R square in two regressions are decreased a lot, regressions become worse.


### (j)

```{r}
confint(lm_12)
confint(lm_12l)
confint(lm_12h)
```
The confidence interval are smaller in lower noise model, higher in higher noise model. So data with less noise can be estimated more precisely because it has less uncertainty.


## 3.14

### (a)

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```
$$ y=2+2x_1+0.3x_2+\epsilon $$, the $\beta_0$ is 2, $\beta_1$ is 2, $\beta_2$ is 0.3.


### (b)

```{r}
cor(x1,x2)

ggplot(data.frame(x1,x2),aes(x=x1,y=x2))+
  geom_point()
```
The correlation between x1 and x2 is 0.835.


### (c)

```{r}
lm_13 <- lm(y~x1+x2)
summary(lm_13)
```
The estimated $\beta_0, \beta_1, \beta_2$ are 2.13, 1.44, 1. They are very different from real coefficients.

The p value of $\beta_1$ is low, which means the estimate of $\beta_1$ is significant, so we can reject null hypothesis H0 : β1 = 0, but p value shows the estimate of $\beta_2$ is not significant, so we can not reject null hypothesis H0 : β2 = 0.


### (d)

```{r}
lm_131 <- lm(y~x1)
summary(lm_131)
```
We can reject null hypothesis H0 :β1 =0 because p value of ß1 is small.


### (e)

```{r}
lm_132 <- lm(y~x2)
summary(lm_132)
```
We can reject null hypothesis H0 :β2 =0 because p value of ß2 is small.


### (f)

The results from (d) and (e) are not contradict, because x2 is generated from x1, it has all information from x1, so x2 can also interpret y as what x1 does.


### (g)
```{r}
x1 <- c(x1, 0.1) 
x2 <- c(x2, 0.8) 
y <- c(y, 6)
```

```{r}
lm_13g <- lm(y~x1+x2)
summary(lm_13g)

lm_131g <- lm(y~x1)
summary(lm_131g)

lm_132g <- lm(y~x2)
summary(lm_132g)
```
The coefficient estimates are not changed a lot, so the new observation has little influence on regressions.

```{r}
par(mfrow=c(2,2))
plot(lm_13g)

par(mfrow=c(2,2))
plot(lm_131g)

par(mfrow=c(2,2))
plot(lm_132g)
```
The new observation is a high leverage point for three regressions, but it is not an outlier for three regressions except y~x1.

